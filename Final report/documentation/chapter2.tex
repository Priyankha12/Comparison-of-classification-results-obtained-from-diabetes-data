
\chapter{LITERATURE SURVEY} 
Extensive research has been done in the area of classification and prediction of diabetes.
\section{Decision Tree}
Decision tree method was used to predict diabetes among patients with and without diabetes [2] using Pima Indian dataset taken from UCI Machine Learning Repository which has data collected from females over the age of 21. Two phases were involved. In the first phase data pre-processing techniques like attribute identification and selection, handling missing values and numerical discretization is applied. The transformed data is then used for classification based on the J48 algorithm which is a decision tree learner and an implementation of Quinlan C4.5 in Weka Knowledge Explorer, in phase two. The same software was used to test the model constructed by applying 10-fold cross validation and an overall accuracy of 78.1768\% was achieved from the results. Only one decision tree algorithm- J48 was used to classify the patients in the dataset. Moreover, only the accuracy and error rates were considered for evaluation of the performance.\par\noindent
RapidMiner tool was used in PIMA Indian dataset [6]. Missing value for the attributes Plasma-Glucose, DiastolicBP, and BMI were removed followed by outlier analysis. Numerical attributes were discretized before normalization. After normalizing the data, two predictive models are built namely decision tree and ID3.
A decision tree can be learnt by splitting the source data set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner. The recursion is completed when splitting is either non-feasible or a singular classification can applied to each element of the derived subset. A random forest classifier uses a number of decision trees, in order to improve the classification rate. It was found that this method gives an accuracy of 72\%.
The ID3 Algorithm adopts a greedy (i.e., non-backtracking) approach where decision trees are recursively constructed in a top-down divide and conquer manner. It starts with a training set of tuples and associated class labels. The training set is recursively partitioned into smaller subsets as the decision tree is being built and this method produces an accuracy of 80\% which is greater than the previous method by 8\%. \par\noindent  
Modified version of the ID3 algorithm [8] was used to classify the dataset which is continuous and bound into a specific range. By dividing a continuous feature into k intervals, the ID3 algorithm will create a decision tree that can work well on the floating point value data set, which cannot be classified in the original ID3. Pima dataset was split into a 75 is to 25 ratio and selects k values as 3, 6, 9, 12, 15, 18 and 20. It is observed that as the number of intervals increases, the decision tree has more choices for the samples to fall into their appropriate categories and hence decreases the error rate of the decision tree learning. However, at a specific number of k (k=12), the decision tree seems to get into the optimal state. The error rate of the decision tree, which has a value of k exceeding the best point, does not decrease as in the previous k. As a result, the error rates for k value 15, 18 and 20 are higher than that of 12.
As the modified ID3 continues increasing the value of k beyond the finest point, the error rate gradually increases. It indicates that there must be an appropriate value of k for a particular feature. The suitable value of k for the Pima Indian diabetes problem, should be around 12 and the accuracy was found to approximately 65\%.

\section{ANN}
Classification of diabetes data was performed on Pima Indian Diabetes dataset using Complex-Valued Neural Networks (CVNN) and Real-Valued Neural Networks (RVNN) based parametric modelling approaches [1]. Complex data normalization technique converts the real valued input data (RVD) to complex valued data (CVD). CVNNs learn the relationship between the input and output data during training and the Complex-valued Autoregressive (CAR) model is extracted from the complex-valued weights and coefficients of trained network. Finally, classification of the CAR or RVAR model coefficients is performed. Further, the effect of data normalization techniques, activation functions, learning rate, number of neurons in hidden layer and the number of epoch is studied and evaluated in this paper.

The first stage is data formatting (structural representation of the data in accordance to the model format) and normalization (band limiting of the data). Complex data normalization converts RVD to CVD and both kinds of data are studied. The data normalization techniques used are- z-score normalization (NT1), min-max data normalization (NT2), complex data normalization (NT3) and unitary data normalization (NT4). NT3 data normalization maps input RVD to a complex valued interval [0,$\pi$] and retains the relational and spatial properties that exist between two entities in RVD.
The ANN-based AR model classification technique is developed.
The model coefficients obtained from the trained network is grouped into regions, and hence the model output classifier is built.
The performance analysis in this paper include- True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN), Accuracy (Acc) and Time of Completion (TOC).
Effect of data normalization, number of neurons in the hidden layer, learning rate, number of epoch and activation function on the accuracy of the proposed model is observed.
It was found that the accuracy obtained using hyperbolic tangent is better than that obtained using sigmoid activation function.
The proposed model performed better using complex data normalization technique with hyperbolic tangent (tanh) as activation function.
The use of very low learning rate increased the TOC while high learning rate led to instability in the algorithm.
Increasing the number of neurons in the hidden layer did not have a significant effect on accuracy of the model for tanh activation function using NT1, NT2 and NT3 techniques. For NT4, increasing the number of neurons in the hidden layer increased the accuracy.
It was observed that, increasing the number of epochs led to an increase in TOC without affecting accuracy.
TOC using tanh activation function was always greater than TOC using sigmoid for all data normalization techniques, implying a trade off between accuracy and TOC in using tanh or sigmoid.
It was concluded that the proposed ANN-based AR model produced accuracy in the range of 80.65\%-81\% which compare favorably with existing classification techniques for the same dataset. \par \noindent
Four models namely Artificial Neural Network (ANN), Decision tree (DT), Support Vector Machine (SVM) and Logistic Regression (LR) was performed on Pima Indian diabetes dataset [3]. The first stage of pre-processing used was to discard the cases having one or more missing attribute values from the original 768 cases. Then the remaining missing values are imputed using K-Nearest Neighbor (KNN) algorithm. After imputing the data outliers are detected using box plot and the outliers are eliminated. Following which the hidden patterns in the dataset are extracted using K-means algorithm and the misclassified samples are eliminated and the correctly classified samples are given as inputs to the ANN, DT, SVM, LR with a 70-30 split. The developed models cascaded k-means combined with LR and k-means combined with ANN give an accuracy above 98\%. The model k-means and SVM achieved an accuracy 97.13\% and the model K-means and DT achieved an accuracy of 97.99\%. As compared with accuracy the proposed model cascaded k-means and LR is best among the methods considered. \par \noindent   
A hybrid Adaptive Neuro-Fuzzy Inference System (ANFIS) was used for classifying diabetes in the Pima Indians Diabetes dataset [5]. It was implemented using ANFIS Fuzzy Logic Toolbox and MATLAB Toolbox. The performance measures were analyzed in terms of specificity, precision and sensitivity.
ANFIS is a hybrid adaptive neural network that has the capability to adjust the membership functions and the consequent parameters during learning process, by applying an optimization method. In this case, mean square error between current neuro-fuzzy system and its proposed output was used as the minimized criteria function.
A five-layer ANFIS structure had the following functions:\newline
Layer 1: Every node was an adaptive node with a node function \newline
Layer 2: Every node was fixed, whose output was a product of all incoming signals and represented the firing strength of a rule. \newline
Layer 3: Every node was a fixed node. The ith node calculated the ratio of the ith rule’s firing strength to the sum of all rule’s firing strengths. \newline
Layer 4: Every node was an adaptive node with a node function. \newline
Layer 5: Had a single, fixed node which computed the overall output as summation of all incoming signals. \newline
The training set used had 80\% of the data, and the test set had 20\% of data. Linear function coefficients were calculated by least squares method RMS. Error signals were back-propagated and the antecedent parameters of membership function were updated using gradient descent method.
The training process yielded the rule base. The dataset selected for testing included six variables- Plasma Glucose Concentration, Diastolic Blood Pressure, Triceps skin fold thickness, 2-Hour serum insulin, Body mass index and Diabetes pedigree function.
The quality of classifier was measured from confusion matrix that contains, True positive (TP), True negative (TN), False positive (FP) and False negative (FN) cases. Based on these values accuracy, sensitivity, specificity and precision values were computed. It was observed that the proposed neural network produced an accuracy of 85.35\% for training data and 84.27\% for testing data. \par \noindent
Various pre-processing techniques were used to compare the performance of Artiﬁcial Neural Network (ANN) to classify the patients in Pima Indian dataset [7]. The impact of pre-processing is highly relevant in this context because medical datasets generally have missing values but ANNs require complete set of data for accurate classiﬁcation. Computer simulations were carried out using MATLAB to analyze the performance of four approaches of pre-processing namely- (1) omit samples, (2) replace with zeros, (3) replace with mean and (4) replace with KNN with respect to epochs. Accuracy was tremendously improved when using KNN and mean with PCA preprocessing method. \par \noindent
Two models, namely, multilayer neural network trained by Levenberg-Marquardt (LM) algorithm and a probabilistic neural network were built on Pima Indian diabetes dataset [10].
The first stage was construction of a multilayer neural network (MLNN) structure trained by LM algorithm. The widely recognized back-propagation (BP) algorithm suffers from a slow convergence rate, and leads to suboptimal solutions because it applies the steepest descent method to update the weights. Hence, LM algorithm which provides faster convergence and better estimation results was used. However, LM can cause memorization effect when the overtraining occurs. This effect decreases the generalization and performance may not be improved for untrained test sets. In order to address this drawback, the study proposed to determine the optimum trained neural network using the maximum accuracy value of the test data.
At the second stage, a probabilistic neural network (PNN) was built. It is a model based on competitive learning with ‘winner takes all’ strategy and the core concept based on multivariate probability. The PNN used feature vector as eight-dimensional real valued input vector, with a single hidden layer (radial basis layer) which are fully interconnected to an output layer of two units which are indices of two classes.
Two kinds of validation techniques were used. The conventional one training set and one test set validation; and 10-fold cross-validation technique was applied and accuracy was compared to other neural network models. For the conventional validation method, the first 576 cases were used as the training set and the remaining 192 cases were used as the test set. In k-fold cross-validation method, the dataset is divided into k mutually exclusive subsets. The training and testing is done k times. In each iteration one of the folds is taken as test data and remaining (k-1) folds are used as training set. Thus, k different test results are obtained for each train-test configuration and average of these results give the test accuracy of the algorithm.
An accuracy of 79.62\% was obtained for MLNN with LM and 78.05\% was obtained with PNN when 10-fold cross-validation was used. The same algorithms (i.e. MLNN with LM and PNN) gave 82.37\% and 78.13\% accuracies respectively with conventional validation technique.

\section{General}
The performance of multiple ensemble techniques like Majority voting, Adaboost, Bayesian Boosting, Stacking and Bagging were compared [4]. Three types of decision trees- ID3, C4.5 and CART were used as base classifiers. Two benchmark datasets from UCI and Biostat repository were used and the experimental results show that Bagging ensemble technique shows better performance when compared to a single classifier as well as other ensemble techniques. Since ensemble classifiers are more accurate in performance and prediction than standalone classifiers and that they produce more flexible structure, it can be applied in the field of data mining for diabetes diagnosis. The methodology proposed involves implementation of the three decision trees- (ID3, C4.5 , CART) and use these as a base classiﬁer for various ensemble classifiers which generate different performance while evaluating parameters like accuracy, sensitivity, specificity and f-measure using RapidMiner tool. A future scope of applying the aforementioned technique for other disease datasets like breast cancer, heart disease and liver disease was provided. Moreover, heterogeneous base classiﬁers can be applied to increase the performance. \par \noindent
A survey of various data mining methods applied to diabetes data analysis and prediction of the disease was discussed [9]. It describes three broad methodologies to approach the same. Firstly, several association rule based algorithms in this context are as follows:
\begin{itemize}
\item A new approach to generate association rule on numerical data by applying pre-processing techniques like equal interval binning and then applying Apriori Association Rule algorithm to generate the rules.
\item An advanced and reliable technique to identify risk factors behind the diabetes to generate association rules and identify the relationship between diabetes and other diseases.
\item Combination of decision tree and association rule approach to build different decision trees, convert them into different set of rules and then further reduce and filter it. The final conclusion is that the set of rules from decision tree was much smaller than the results of association rules.
\item Theory of association rule of Diabetes Mellitus (DM) patients data with renal, Ophthalmic, Neurological and Peripheral Circulatory complications.
Secondly, various clustering and classiﬁcation based algorithms for diagnosis of diabetes are as follows:
\item Combination of K-means clustering with various classification algorithms like SMO (Simple Minimal Optimization), Naive Bayes, Bagging, AdaBoost, J48, Rotation Forest and Random Forest.
\item Hybrid model for classifying PIMA Indian dataset. Identify datasets and incorrectly classiﬁed instances are eliminated using K-means clustering. C4.5 decision tree is done on correctly clustered instances and resultant dataset is tested using 2 methods- dividing training and testing data into 60-40 ratio and 10-fold cross validation. This method improves accuracy by an order of 19.50\% of classification compared to decision tree C4.5 alone with unprocessed data.
\item Research done on 3 technique- Expectation Maximization (EM), H-means clustering and Genetic Algorithm(GA) on PIMA dataset.
\end{itemize}
Thirdly, other methods like RapidMiner tool, CART, Neural Network and Artificial MetaPlasticity on Multilayer Perceptron (AMMLP) for the purpose of diabetes prediction and diagnosis were discussed. Finally it can be concluded that from analysis of various presentations and studies done by other researches it is evident that occurrence of diabetes is having a strong relation with
\begin{enumerate}
\item Diseases like wheeze, edema etc.
\item Female pregnant
\item Increase of age
\end{enumerate}
Data mining techniques have to be done more intrinsically to relate diabetes to other diseases for accurate and early detection as a part of future scope. \par \noinent
A homogeneous ensemble technique was used [11]. It ascertains that the ensemble models accuracy is way better than any algorithm alone. The ensemble method used is random forest. The algorithm, performs better when the number of attributes or features is greater than the number of tuples themselves. Random forest algorithm constructs several randomized decision trees and aggregates their predictions for improved accuracy. A fair idea about the unrelated features like Total Cholesterol, High Density Lipoprotein, Stabilized Glucose, High Density Lipoprotein, the Cholesterol/HDL Ratio and the First Systolic Blood Pressure was given, which paves way for dimensionality reduction. Data like age, weight, height, hip, and waist have been discretized. Gini index has been used to determine the split point. The ﬁnal result is presented based on majority voting. Based on individual decision trees, rules and useful relationships may be extracted. Another interesting observation made is the accuracy with respect to data scale. It is seen that random forest method is consistently improving with increase in the proportion of input data. Thus, random forest is suitable for sufficiently large data sets. A homogeneous ensemble technique was discussed, however it is possible to obtain better results using heterogeneous alternatives.

